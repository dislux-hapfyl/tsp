
# Music

Before starting this chapter, I'm expecting you to understand that human brains are biologically evolved computers. They are massive networks of interconnected cause/effect gates, which can dynamically change their behavior (Unlike a silicon computer gate, which has a fixed behavior over time) to learn stuff. These interconnected gates excite each other through electrical impulses. Electrical impulses are the language with which our neurons talk with each other. So, in order to sense something from the external world, some kind of translation is needed, and that's why we have **Senses**.

There are 6 different kinds of ways a human brain can make sense of the external world. Here is the list, we used to call it the 5 main senses, but there are actually 6 (Do not confuse it with the paranormal sixth sense in the fictions!):


| Name    | Organ      | Stimulus                    | Stimulator    |
|---------|------------|-----------------------------|---------------|
| Sight   | Eyes       | Light                       | Physical self |
| Hearing | Ears       | Sound                       | Larynx        |
| Balance | Inner-ears | Acceleration                | ?!            |
| Smell   | Nose       | Chemical substances         | Sweat         |
| Taste   | Tongue     | Chemical substances         | ?!            |
| Touch   | Skin       | Position/Motion/Temperature | Touch/Hit     |

Human brains need to communicate with each other in order to maximize their evolutionary goals, but they are not directly wired to each other (Imagine they were actually connected to each other directly through wires, an interesting question would be, would they then become a single person?) So what is the solution? How can a conscious piece of meat trapped inside a hard skull communicate with other brains? Pretty much like regular computers you have in your home, brains do also have external auxillary hardware connected to them. Eyes, ears, nose, tongue and skin are examples of devices that are connected to the brain to allow it to sense effects of the external world.

After all, our brain's language is the flow of electricity inside denderites and axons. Before reading the answer, let's imagine we are The Creator and we are going to design a communication system for our creatures to transfer data t

In this chapter I would like to talk about one of the pleasurable inventions of human beings, music. You hear it everyday, whether as an artistic piece to please your ears, or the sound you hear from your phone when you press a key. I believe composing and playing sounds is one of the most important applications of a computer, which is very underrated topic in computer science. Musical notes are lego pieces of our mind. Putting the right notes in the right order can manipulate a mind, bringing sadness, happiness, excitement, horror, magic and all kinds of emotions in a human brain. The good news is, sound waves, which are the most primitive components of music, can be easily generated a computer, because there are ways to convert electrical causes to mechanical effects. Guess what kind of effect a computer is good at generating it? With the current technology, it’s not trivial to generate “smell” effects. We will go through the history of sound and we will try to understand it by breaking a music to its most primitive parts, and in the end of this chapter, we will be able to make your hear a symphony of beethoven by writing a program that outputs nothing but numbers.

Understanding music will make you a better programmer, because music itself is all about playing with lego pieces, which is what we programmers basically do everyday.

Thomas Alva Edison, some people think of him as a bad guy who stole Tesla’s ideas. We are not here to judge, anyways, he was an inventor and made inventions through his life that had big impacts in our history and were inspiring many people. Here I want to talk about one of his inventions which I myself find very primitive and impressive, the Phonograph (Nowadays we call them Gramophone too) (1877).

Phonographs, as defined in Wikipedia, are devices that can record and reproduce music. The idea behind it is both very smart and simple. Imagine a needle on a rotating disk, where the needle is connected to something like a loudspeaker, which vibrates when someones speaks near it. The needle drills holes on the disk when it vibrates. As the disk rotates, there will be different holes with different depths over time, effectively recording the intensity of air pressure over time.

There is an organ in our body that can vibrate the air in different rates, larynx. When people speak, a neural effect is effectively causing a mechanical effect, which is vibration of the larynx. (Unrelated to this section, but can't resist to tell you that when you speak, somebody hears, and that hearing will cause neural effects. So human communcations are basically neural transistors, capable of building complicated computers which are able to do weird stuff)

Here is a cause/effect analysis of a phonograph: a mechanical effect (Vibrating air) will cause another mechanical effect (Vibrating needle) which will itself cause another mechanical effect which is drilling holes on the disk.

Now imagine we stop recording, rotate the disk in reverse direction so that the needle is on its original position and start again. The holes on the disk will cause vibration on the needle which will itself cause vibration of the loudspeaker which will cause vibration of the air will cause vibration of our eardrum, which will end up as an electrical signal in our brain.

Here is a very useful engineering principle for you: a rotating disk, or basically anything (Physical or conceptual) that can capture the intensity of a world property, can record it. This tape can later be replayed.

If you look closely, you will figure that there is also something familiar and interesting happening when a phonograph plays a music, the vibration applied on the phonograph needle is not able to vibrate the air around it and create sound, but the vibration is routed to the mechanical loudspeaker and somehow intensified there, before reaching our ears, very similar to an electrical amplifier. That's exactly what happens in a transistor, with the difference that the type of cause and effects in a transistor is electricity.

How do we hear? To answer this question, we have to first figure how a vibration (Mechanical effect) is causing a neural stimulation (Electrical effect) in our brain.

Sound is the result of flowing of a mechanical wave through the air. Air is the context here, in a vacuum environment, sound won’t pass. Humans are able to shake the air in their lungs and generate sound. This sound passes through the air and reaches to ears of other humans. Through this process, neural effects in one brain causes neural effects in another brain. Building a meta computer that we call it “society”. (Though, this is not the only medium, people can record their neural effects on a piece of paper by writing, causing neural effects in humans of the future)

Now I want you to think about the translation process here. How is a neural effect translated into a sound in our system? And how is a mechanical cause converted to a neural effect when the sound reaches our ears? Before analyzing a human larynx, I would like to you how a electrical speaker works. The way an electrical speaker generates sound is actually much simpler than a human. An electrical speaker is basically a electromagnet (A magnet in which magnetic field is generated by the flow of electricity through a coil), that is installed besides another magnet which is connected to an elastic plate. When current flows in the coil, the imposed magnetic field will pull the other magnet and the plate. When the current stops flowing, magnetic field goes away and the plate will go back to its original position. You can see the obvious conversion of an electrical wave into a mechanical wave here.

Another interesting fact is that, the same system can act like an electrical microphone too (Remember a phonograph could both record and play music?). Vibration of air will vibrate the magnet which is connected to the plate, and that magnet will impose an electrical flow inside the coil. Voila!

We now know that a single electrical signal is enough to generate voices and also, a single mechanical signal is enough to hear voices. But unfortuantely, that's not how a brain generates/analyzes sounds. The reason is that analyzing a single signal is too complicated, and the nature has not been smart enough to design a brain that is able to hear with a single signal.

When you are in a party, everybody dancing to a loud music, you still can hear what your friend is saying, which is very amazing, because, sound is a single signal, a single stream of intensities. The loud music and the voice of your friend are combined into a single signal and mixed together. The reason you can hear your friend even in a loud room is the same reason you can recognize the taste of an energy drink inside a cocktail, even when it has too many ingredients (Loud in some sense!), even though everything is mixed together. Because there are different receptors that can recognize different primitive elements of that sense, whether hearing or tasting. In case of tasting, the primitive elements of a taste are probably sweetness, sourness, bitterness and etc. But what is the primitive element of an audio signal?

Joseph Fourier, the french mathematician, described that a function/signal can be expressed as the sum of a series of sine or cosine terms. This means that pure sine functions can be primitive elements that form a signal. If we combine two different sine terms of different frequencies by summing them, we can still get them back after a conversion, which is called a Fourier Transform.

So theoritically, if someone speaks in some frequency range, and someone else speak in another frequency range, our brain will be able to distinguish between these two, even if they speak at the same time. If their frequency ranges are too close, it will become hard for brain to understand what each of them is saying. So, human larynx has evolved in a way so that everyone has a unique frequency range!

Enough talking, let's write a program to generate sounds of different frequencies, and their combinations, and then hear how it sounds. This will help you understand the concept much better!

In order to generate a wave of frequency $f$, you can use this formula:

$sin(2\pi ft)$

There is a program in Linux distributions named `pacat`. PulseAudio Cat's goal is to allow you directly put sound samples on your computer's speaker. It does this at default rate of 44100 samples per second. By default, it gets signed 16bit integer from its standard input and plays it on your speaker by differentiating the voltage applied on the speakers magnet plate. When the sample has lowest possible value (-32768), maximum negative voltage is applied and the plate is pulled, and when the samples has highest possible value (32767), the plate is pushed. A sample of value 0 means zero voltage which means zero magnetic field and the plate remains in its orginal position. Given that the sample ratio is 44100, we should substitute the $t$ variable with $\frac{n}{44100}$ where $n$ is the sample number: $sin(2\pi f\frac{n}{44100})$

```python=
import sys
import struct
import math


# Send sound sample to stdout as a 16-bit signed integer
def put(x):
    sys.stdout.buffer.write(struct.pack("h", int(x * 32767)))


sample_rate = 44100  # Rate per second
step = 1 / sample_rate
length = 5  # Seconds
freq = 440


def f(t):
    return math.sin(t * 2 * math.pi * freq)


t = 0
for _ in range(sample_rate * length):
    put(f(t))
    t += step
```

`put(x)` function is getting a floating point value $-1 \leq x \leq 1$ as its input and putting a signed short integer between $-32768 \leq s \leq 32767$ into the stdout. It is the main gate which allows us to convert an electrical cause to a mechanical effect, by vibrating a magnetic plate in your computer!

Redirect your script's output to the `pacat` program and hear the voice of your program:

```bash
pacat <(python3 music.py)
```

Now let's combine two frequencies and hear how it sounds!

```python
def f(t):
    a = math.sin(t * 2 * math.pi * 440)
    b = math.sin(t * 2 * math.pi * 660)
    return (a + b) / 2
```

We are taking an average from `a` and `b` so that the output value remains between -1 and 1.

If you hear this function, you will actually notice that it consists of two sounds. Your brain can successfully decomposite the output wave into two sounds, and this is amazing! The reason that you can recognize the 440Hz and 660Hz sound in the output of this script is the same reason you can hear your friend in a loud room full of noise, your brain is able to decouple sounds with different frequencies.

Now that we are able to generate sounds of different frequencies, I want you to do an experiment. Try generating frequencies that are of the form $2^nf$. E.g. try hearing these frequencies: $440, 880, 1760, 3520, \dots$

```python=
def f(t):
    sec = t * sample_rate
    if sec < 1:
        return math.sin(t * 2 * math.pi * 440)
    elif sec < 2:
        return math.sin(t * 2 * math.pi * 440 * 2) # 880Hz
    elif sec < 3:
        return math.sin(t * 2 * math.pi * 440 * 2 * 2) # 1760Hz
    elif sec < 4:
        return math.sin(t * 2 * math.pi * 440 * 2 * 2 * 2) # 3520Hz
```

In the code above, we are generating different frequencies through time. We start with 440Hz, and we double the frequency every 1 second.

Hear them carefully, and then compare how they sound when their frequency is doubled. Try a different coefficient. Generate sounds that are of the form: $1.5^nf$: $440, 660, 990, 1485, \dots$

We can discover something very strange and important here. Sounds that are generated when the frequency is doubled each time, are very similar to each other (At least, to our brain). While in the second experiment, sounds seem to be different with each other. If the sounds that are generated in the first experiment are similar, then what makes them different?

Let's try another experiment (This time without coding!). Play one of your favorite musics and try to sing on it. Try to sing on it with lower and higher pitches. Even though you have changed your voice, your singing still "fits" the song. By singing on a song with higher pitch, you won't shift your frequency by some number, but you will actually multiplying it by a power of two. A man and woman with totally different frequency ranges can both sing on the same song, but both of the singings will "fit" on the song, as long as the frequencies are multiplies of powers of 2. I think it's now safe two say, frequencies that are twiced every time, have same feelings.

Let's start with 300Hz. We calculate powers of two multiplied by the base frequency. Here is what we get:


`..., 37.5Hz, 75Hz, 150Hz, 300Hz, 600Hz, 1200Hz, 2400Hz, ...`

Let's agree that they are all basically the same "sound", and let's put a name on them. Let's call them S1.

Whatever frequency we take from the range 300Hz-600Hz, there will also exist the corresponding "same-feeling" sound in ranges 75-150, 150-300, 600-1200, 1200-2400 and etc.

Conversely, we can also conclude that any random frequncy have an corresponding "same-feeling" frequency in 300Hz-600Hz range. In order to find its same-feeling frequency, we just need to halve/double it, until it gets between 300Hz-600Hz. (E.g. let's start with 1500Hz, we halve it 2 times and we get 375Hz which exists in the range, so 375Hz is the equivalent same-feeling sound of 1500Hz). We know there are infinite number of frequencies between 300Hz-600Hz. A single S1 is not enough for creating beautiful pieces of music. We could only build Morse codes out of the single sounds! So let's discover and define other sounds in the same range.

Imagine we define S2 as a sound that is in the middle of two S1s. Given that the self-feeling sounds are repeated in a loop, here is what we should expect:

```
S1 - 75Hz
S2 - ?Hz
S1 - 150Hz
S2 - ?Hz
S1 - 300Hz
S2 - ?Hz
S1 - 600Hz
S2 - ?Hz
S1 - 1200Hz
```

What is the definition of middle? Let's say, $C$ is in the middle of $A$ and $B$, when the distance between $C$ and $A$ is equal with the distance between $C$ and $B$. How is *distance* defined in case of frequencies? As a first guess, we can take the average of two S1 frequencies in order to find the S2 frequency.

```
S1 - 75Hz
S2 - (75 + 150)/2 = 112.5Hz
S1 - 150Hz
S2 - (150 + 300)/2 = 225Hz
S1 - 300Hz
S2 - (300 + 600)/2 = 450Hz
S1 - 600Hz
S2 - (600 + 1200)/2 = 900Hz
S1 - 1200Hz
```

As you can see in the calculations, you can get other S2 sounds by multiplying/dividing any S2 sound by powers of 2, just like what happened with S1. So the derived S2 frequencies are all valid and all same in terms of their feelings.

This is not the only way one can define the "middle" frequency here. Imagine instead of taking the average, we multiply the numbers, and take their square-root instead. This is called an "multiplicative" average, or a geometric mean (As opposed to an arithmetic-mean that we used in the first example):

$m=\sqrt{ab}$

We know that the frequencies are doubled in each iteration, so $b=2a$. Substituting this in the equation we get: $m=\sqrt{2a^2}=a\sqrt{2}$

```
S1 - 75Hz
S2 - 75 x sqrt(2) = 106.06Hz
S1 - 150Hz
S2 - 150 x sqrt(2) = 212.13Hz
S1 - 300Hz
S2 - 300 x sqrt(2) = 424.26Hz
S1 - 600Hz
S2 - 600 x sqrt(2) = 848.52Hz
S1 - 1200Hz
```

As you can see, both of these definitions of "middle" give us valid sounds (By valid, I mean they are frequencies that are same-feeling), that indeed are in the middle of the S1 frequncies. The first definition of "middle" gives us numbers that have same arithmetic distance with the S1 frequencies ($m-a=b-m$), while in the second definition, the results are numbers that have same multiplicative distance with the S1 frequencies ($\frac{m}{a}=\frac{b}{m}$). Unfortunately, each of these defintions of middle is giving us different middle frequencies.

So, the million dollar question is, which one is the more appropriate defintion of "middle" here?

Let's look to those numbers again, this time without considering the middle frequencies. Take 3 consecutive S1s in a row. (E.g `300Hz 600Hz 1200Hz`). By definition, if we take 3 consecutive sounds, the second one is in the middle of the first and third sound. If we forget about our artificial middle frequencies and take a stream of S1s as our reference sounds, are the middle frequencies the artithmetic or the geometric mean of the other frequencies? 

Let's say our 3 consecutive sounds are $2^ka$, $2^{k+1}a$ and $2^{k+2}a$. How can we calculate the middle frequency if we only had the values for the first and the third frequency? Let's try both artithmetic and geometric means.

$m_a=\frac{2^ka+2^{k+2}a}{2}=\frac{2^k(a+4a)}{2}=5\times 2^{k-1}a$

$m_g=\sqrt{2^ka \times 2^{k+2}a}=2^{k+1}a$

Obviously, geometric mean is the natural definition of "middle" in a raw stream of S1 frequencies, so it makes a lot of sense to define middle as the geometric mean.

Good to mention that another bug also happens when we try to define middle frequencies with arithmetic means. Although S2 resides in the middle of two S1s, S1s do not reside in the middle of S2s! $\frac{112.5 + 225}{2} \neq 150$. This does not happen when we use geometric means: $\sqrt{106.06 \times 212.13}=150$.

Fortunately, we have been able to find a way to halve frequency ranges and create new same-feeling sounds out of them! But are 2 sounds enough for making a song? Well, we don't have to stop here. We can still find the middle of a S1 and a S2 and define a new S3 sound. The middle of the gap between an S2 and the next S1 can also define a new S4 sound. We can build infinitely many sounds just by halving the frequency gaps!

Enough explanation. Given that you now know how same-feeling sounds are derived, let's talk about the standard frequencies that are used in the music we hear everyday. Just like how we defined a base frequency S1 for deriving new sounds, music composers also defined a base frequency (That is 440Hz) and derived the other sounds out of it. They called their base frequency **A**, splitted the gap between two consecutive **A**s into 12 different sounds (Probably because 12 was big enough to make big variations of music and small enough so that the sounds remain distinguishable for human ears) and named the 11 other derived sounds as `A# B C C# D D# E F F# G G#`. How can you take 12 sounds out of a base frequency? New sounds can be derived by multiplying the previous sound by $\sqrt[12]2 \simeq 1.05946$. Starting from $A$ sound, if you do this 12 times, you get back to the next same-feeling A sound again, because $\sqrt[12]2^{12}=2$.

```
A = 440Hz
A# = 440 * 1.05946 = 466.16
B = 440 * 1.05946^2 = 493.88
C = 440 * 1.05946^3 = 523.25
C# = 440 * 1.05946^4 = 554.37
D = 440 * 1.05946^5 = 587.33
D# = 440 * 1.05946^6 = 622.25
E = 440 * 1.05946^7 = 659.25
F = 440 * 1.05946^8 = 698.46
F# = 440 * 1.05946^9 = 739.99
G = 440 * 1.05946^10 = 783.99
G# = 440 * 1.05946^11 = 830.61
```

Let's get our hands dirty and play some melodies with these sounds!

`E E E _ E E E _ E`


-----

After playing some melodies by generating different frequencies in a sequence, you might nag that the notes are very pure, emotionless, and not at all close to what we hear when we press a key on a piano, and you are right. We are generating pure sine-waves, meaning that what we hear contains only a single frequency. If you analyze the sound coming out of a musical instrument, you will notice that not only it contains frequencies other than the main frequency, but also the strength of those frequencies change over time.

It's not easy to generate a piano sound using pure sine frequencies, but there are other ways we can oscillate a loud-speaker, letting us to get more creative and generate sounds that are more interesting and weird.

Imagine sharp jumpings to -1 and +1 instead of smoothly going up and down like a sine wave. This will make a square wave that feels very differently. Square-waves have vintage feeling, since the game consoles back then didn't have 

**Different oscillators - Sawtooth - Square - Triangle**
**ADSR**


How can we emulate an ear in a computer


$X_k=\sum\limits_{n=0}^{N-1}{x_n.e^{-i2\pi kn/N}}$


Reverb, sound tracing and …

Sound is data

Distribute sound, radio stations

Antenna heinrich hertz 1887

Telephone 1896 grahambell

Alec Reeves - inventor of pcm 1930s

James Russel - 1978 - store music on optical

Hdd 1956 IBM

Digital data transfer samuel morse early 19th

1950 computers connected dedicated communication lines

Aliasing claude shannon

Humans love to store and copy. If you can copy or store something, you can transfer it. You can write your thoughts on a letter and then give it to your lover to read

Humans dream: being able to share their past experience to the present and future. To the people that are not near them. To people in the future even after they physically die, because life has evolved in a way to keep you alive as long as possible, sometimes the conceptual version of you, and not the physical you. That’s probably why people proudly compromise their lives for something they believe in, because it will make them eternal. Life is strange.